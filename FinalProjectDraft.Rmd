---
title: "STAT0218 Final Project"
author: "Ellie Suit, Andy Atallah, and Ai Hattori"
date: "2023-12-06"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load packages, warning = F, message = F}
# libraries 
library(tidyverse)
library(survival)
library(rpart)
library(rpart.plot)
library(dendextend) # hierarchical clustering
library(lubridate) # hierarchical clustering
library(kernlab) # SVM
library(e1071) # SVM
library(ranger) # RF
library(caret) # test of SVM
library(LiblineaR) # caret SVM testing
library(kernlab) # caret SVM testing
library(survivalsvm) # functional SVM 
library(rms) # validate R-part

# set a seed
set.seed(1)
```

# Introduction

In this study, we make predictions on the factors that influence tree survival amongst seedlings to determine which aspects contribute the most to tree health. In particular, we analyze the plant-soil relationships to characterize how qualities of the soil and properties of a given tree influence each other. We implement survival analysis to identify the most important variables and use a variety of statistical learning models to compare and contrast the accuracy of each method. We present a report that analyzes how the cox proportional hazards regression model (coxph), random forest, survival trees, and support vector machines (SVM) compare to each other in their ability to understand the plant-soil relationships. 

```{r load data, warning = F, message = F}
library(randomForest)
library(caret)
trees <- read_csv("Tree_Data.csv")
#trees <- read_csv(file.choose())
```

# EDA: 

The dataset in this report comes from the article "tree seedling functional traits mediate plant-soil feedback survival responses across a gradient of light availability" by Wood et al. In the article, the researchers conducted a field experiment, "consisting of four tree species, seven soil sources (sterilized conspecific, live conspecific, and five heterospecific), and a gradient of forest understory light levels (low, medium, and high)." 

As Fig. 1 shows, each of the four species has a similar number of trees. 

```{r}
ggplot(data = trees, mapping = aes(x = Species)) + 
  geom_bar(fill = "lightblue") +
  labs(y = "Count", x = "Tree species",
       title = "Figure 1. Number of trees by species")
```

In the research, two controlled conditions for the seedlings were soil sources and light availability. For soil sources, if the soil cores were collected beneath a tree of the same species as the seedling, it is called *conspecific* soil. If a conspecific soil was sterilized, then it is called *sterilized*. If the soil core was collected from beneath a tree of a different specie than the seedling, then it is called *heterospecific}. Fig. 2 shows the distribution of the soil sources segmented by tree species. In Fig. 2, within each type of soil source, tree species are nearly evenly distributed. However, the figure also indicates that Heterospecific soil was more common than Conspecific or Sterilized. 

```{r}
# change the order of levels in Conspecific to make the plot easier to understand
tree_EDA <- trees %>%
  mutate(Conspecific = factor(Conspecific,
                              levels = c("Conspecific",
                                         "Sterilized",
                                         "Heterospecific")))

ggplot(data = tree_EDA, mapping = aes(x = Conspecific, fill = Species)) + 
  # geom_bar(position = "fill") +
  geom_bar() +
  labs(y = "Count", x = "Soil source",
       title = "Figure 2. Soil sources by species")
```

Fig. 3 does the same thing as above for the light levels. In the data collection process, light availability was initially measured as a continuous variable by using Indirect Site Factor (SF) which is the proportion of solar radiation at a given location relative to an open site (https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0293906#sec002). Then, the range of the ISF value was split into three partitions of equal length and three categories of light level (low, medium, and high) were created. Fig. 3 is a distribution of those three light levels segmented by species. Similar to Fig. 2, within each light availability, the number of the seedlings is about the same for all the four species. Yet, more seedlings had the low or medium light level than high. 

How to manually order x-axis on bar chart: https://community.rstudio.com/t/how-to-manually-order-x-axis-on-bar-chart/9601 was used for the following code.
```{r}
trees %>% 
  mutate(Light_Cat = factor(Light_Cat, 
                                    levels = c("Low", "Med", "High"))) %>% 
  ggplot(mapping = aes(x = Light_Cat, fill = Species)) +
  geom_bar() + 
  labs(y = "Count", x = "Light level",
       title = "Figure 3. Light levels by species")
```

In the dendrogram in Fig. 4, there is no split with only one terminal node, which suggests that there is no outlier that should be removed for data analysis.

# Quantifying Results

In this study, we use the Harrelâ€™s concordance index (c-index) to evaluate the quality of our statistical learning models. This survival metric is used to quantify how well the predicted risk score correlates with the survival time. In particular, the c-index prediction error is determined by 1-C, where C is the c-index. This metric takes on a value between 0 and 1 and quantifies how well the model accurately ranks two random observations in terms of survival. Namely, a c-index of 0.5 is no better than random guessing and a c-index of 0 is evidence of a perfect model. Mathematically, the c-index is defined as 

$ C = \frac{\sum_{i,i':y_i>y_i'}I(\hat{n_{i'}}>\hat{n_{i'}})\delta_{i'}}{\sum_{i,i':y_i>y_i'}\delta_{i'}}$,

where $I$ represents the indicator variable, $\hat{n_{i'}}$ and $\hat{n_{i'}}$ represent a pair of observations, and $\delta_{i'}$ is the status variable, taking on a 0 value if the $i'^{th}$ variable is censored and 1 if the $i'^{th}$ variable is uncensored. The indicator variable $I$ similarly takes on a binary value of either 0 or 1, where $I = 1$ if $\hat{n_{i'}}>\hat{n_{i'}}$ (the hazard for the $i'^{th}$ observation is larger than the $i^{th}$ observation) and $I = 0$ otherwise. Practically, this means that the c-index will take on a larger value when it correctly predicts that $\hat{n_{i'}}>\hat{n_{i'}}$, resulting in an error ($1-C$) close to 0. 

As a result, this metric appropriate for defining the efficacy of our model in predicting survival amongst trees. 


# Principal Component Analysis for Random Forest

In this section, we first calculated principal components of numerical variables that may have impacted tree's survival and built a random forest model that used them as a predictor variable for the survival object of Time (the number of days at which time the seedling died or the experiment ended) and Event (status of each seedling at a given time; 0 for alive and 1 for dead).

Because PCA only works with numerical variables, we first removed categorical variables shown in below. 

** Species
** Light Cat
** Soil: Species from which the soil core was taken 
** Adult: individual tree that soil was taken from
** Sterile: whether the soil was sterilized or not
** Conspecific: whether the soil was conspecific, heterospecific, or sterlized conspecific
** Myco: Mycorrhizal type of the seedling species (AMF or EMF)
** SoilMyco: Mycorrhizal type of the species culturing the soil (AMF or EMF)
** Event: this was our outcome variable

After that, we also removed four numerical variables: No, Harvest, Alive, and Time. The variable No is a unique ID attached to each seedling, thus, it should not have influenced tree survival. Moreover, the variables Harvest and Alive are related to what happened to seedlings that were still alive at the end of the experiment, so we reasoned that they would not have impacted tree survival. Lastly, the variable Time was removed from the calculation of loading scores because it was our outcome variable.

In short, we calculated loading scores by using the following variables. 

* Plot
* Subplot
* PlantDate
* Light ISF: the amount of light reaching each subplot at a height of 1m
* Core: Year the soil core was removed from the field
* AMF: Percent arbuscular mycorrhizal fungi colonization on the fine roots of harvested seedlings
* EMF: Percent ectomycorrhizal fungi colonization on the root tips of harvested seedlings
* Phenolics: nmol Gallic acid equivalents per mg dry extract
* NSC: percent dry mass nonstructural carbohydrates
* Lignin: percent dry mass lignin
* Census

## Find loading score 
```{r}
# convert PlantDate in character format to date time object
tree_pca <- trees %>% 
  mutate(PlantDate = mdy(PlantDate)) %>% # change to standard format(Year-Month-Day)
  mutate(PlantDateYear = year(PlantDate),
          PlantDateMonth = month(PlantDate),
          PlantDateDay = day(PlantDate)) %>%
  select(-PlantDate)

# select necessary variables
tree_pca <- trees %>%
  select(-c(No, Harvest, Alive)) %>%
  select_if(is.numeric) %>%
  na.omit()

colnames(tree_pca)

# scale data and conduct PCA
pca1 <- prcomp(tree_pca %>% select(-c(Event, Time)), scale = TRUE) # remove what we are predicting (Event and Time)

# the first PC (direction of the data along which the observations vary the most)
# sort(pca1$rotation[,1])
```

The scree plot below indicates that the first seven principal components explain more than 80% of variance in the selected numerical variables. It also shows that the eighth and ninth principal components explain less than 10% of variance in those numerical variables. 

```{r}
# Variance explained by each component
pca1$sdev^2/ncol(tree_pca %>% select(-Event)) 

plot(1:length(pca1$rotation[,1]), 
     cumsum(pca1$sdev^2/ncol(tree_pca %>% select(-Event))),
     xlab = "Number of principal components",
     ylab = "Cumulative % of variance explained") 
title("Scree plot for PCA")
```

## Build a random forest model to predict survival object of Time and Event by loading scores

After finding loading scores, we used them to build a random forest model that predicted the survival object of Time and Event. We chose random forest as our supervised learning technique because we did not know which loading score(s) had the most importance in predicting the survival object. 

```{r}
# Create a vector of loading scores from all the PCs
loading_score_vec <- pca1$x

# Create a dataset of loading scores, Event, and Time
tree_subset <- data.frame(loading_score_vec) %>%
  mutate(Event = tree_pca$Event,
         Time = tree_pca$Time) 

# Survival tree (single tree)
# surv_tree_1 <- rpart(Surv(Time, Event) ~ .,
#                      data = subset2)
# rpart.plot(surv_tree_1, cex = 1)

# Build a random forest model 
rf_test<- ranger(Surv(Time, Event) ~., 
             data = tree_subset,
             importance = 'impurity')
rf_test

# Plot variable importance
barplot(rf_test$variable.importance,
        xlab = "Variable",
        ylab = "Importance")
title("Variable importance for the random forest model with loading scores")

# Alternative: rfsrc function
# rf_test2 <- randomForestSRC::rfsrc(Surv(Time, Event) ~.,
#                   data = subset2,
#                   importance = TRUE)
# rf_test2
# rf_test2$importance
```

Prediction error: "Prediction error is calculated using OOB data. [...] For survival, prediction error is measured by 1-C, where C is Harrell's (Harrell et al., 1982) concordance index. Prediction error is between 0 and 1, and measures how well the predictor correctly ranks (classifies) two random individuals in terms of survival. A value of 0.5 is no better than random guessing. A value of 0 is perfect." (https://www.rdocumentation.org/packages/randomForestSRC/versions/1.0.0/topics/rfsrc)


## Andy: Survival function

After preparing data, we can use the `Surv` function on the dataset to create a survival object to be used in various functions. We supply the time and event variables (Time and Event respectively) for survival analysis.

```{r Data preparation}
#tree <- read.csv("Tree_Data.csv") 
tree <- trees

# Remove unwanted variables
tree <- tree %>% select(-No, -Harvest, -Alive, -Adult)

# Convert all NA EMF values to 0 with researchers' blessing
tree$EMF[is.na(tree$EMF)] <- 0

# Check for remaining NA values
str_which(tree$EMF, "NA")

# convert PlantDate in character format to date time object
tree <- tree %>% 
  mutate(PlantDate = mdy(PlantDate)) %>% # change to standard format(Year-Month-Day)
  mutate(PlantDateYear = year(PlantDate),
          PlantDateMonth = month(PlantDate),
          PlantDateDay = day(PlantDate)) %>%
  select(-PlantDate)

# Factor variables
tree <- tree %>%
  mutate(Species = factor(Species)) %>%
  mutate(Soil = factor(Soil)) %>%
  mutate(Conspecific = factor(Conspecific)) %>%
  mutate(Subplot = factor(Subplot)) %>%
  mutate(Light_Cat = factor(Light_Cat)) %>%
  mutate(Sterile = factor(Sterile)) %>%
  mutate(Myco = factor(Myco)) %>%
  mutate(SoilMyco = factor(SoilMyco))
  
# Create a survival object
tree_surv <- Surv(time=tree$Time, event=tree$Event)
```

The survival object can first be used in the `survfit` function. We first implemented this function in class with no covariates, which was indicated with a `1` in the formula term reserved for predictors. The resulting survival curve shows an extremely slight decrease in estimated probability of survival at about 15 days and a sharp decrease at about 25 days. Quick exploration of relevant variables can be achieved via Kaggle. Indeed, just 3 observations carry time values of 14, and all of these seedlings seem to have died based on their event value of 1. Many more trees have a time value of 24.5 days, corresponding to a census value of 7. There are no census values less than 4, which signifies that the researchers did not harvest nor note the death of any seedling prior to 14 days into the growing season.

Using the rationale that decreases in survival probability relate to censuses when the researchers assessed the mortality of trees and harvested certain individuals, other periods of heightened risk include 42 days (census 12), 45 days (census 13). By this stage of the growing season, probability of survival has decreased to approximately 60%. The probability decreases to approximately 30% before leveling out for the final 40-odd days of the growing season. Due to the fact that seedling survival was not observed each day, it is not likely that all trees which were noted to have died at census 13 died on day 45; some may have died on days 43 and 44. That is the nature of the data, however, and it was noted in class that survival curves take on a stepwise shape.

In general, this curve indicates that survival is not likely to be compromised at the very start of the growing season and that surviving plants enjoy a risk-deficient period of time about 80 days into the growing season where mortality is relatively unlikely. The most risky periods for the study trees are found from approximately 20 to 45 days into the season; probability of survival decreases by about 40% in this period and 30% in the following 35 days. 

```{r Survival curve}
# Survival curve with no covariates
surv_fit_tree_base <- survfit(tree_surv~1)

# Survival curve
plot(surv_fit_tree_base)
```

We can also try fitting covariates to the survival curve. (Coming later)

```{r}
# Survival curve with covariates
```

The Cox proportional hazards model is also possible to implement using a survival object. Using variables which have been selected to present a signficant p-value. (Will describe more later)

```{r Coxph}
# Coxph
coxph_tree <- coxph(Surv(Time, Event) ~ Species + Conspecific + SoilMyco + Phenolics, Sterile,
                      data=tree)

coxph_tree

# The risk is only 67% of the original for a 1 unit increase in phenolics
# Good vars: Species, Con, SoilMyco, Phenolics
```

## Andy: SVM with survival object

We have used support vector machines for datasets with many possible predictor variables. It thus seems prudent to try to use SVM for this problem. Upon testing SVM with the `caret` package, which we have done before in class using a value of "svmLinear2" for the `method` argument, the `train` function does not seem to be able to work. This is likely because we have passed in a survival object as `x` instead of the typical single predictor, for the error is "Invalid operation on a survival time". Altering the method to one of the other SVM methods included as options in caret also does not work, and the `e1071` package itself predictably fails as well. 

```{r SVM}
# Caret package does not support survival models
svm_tree <- train(Surv(Time, Event)~., data=tree %>% na.omit(), method="svmLinear2")

# e1071 package also throws an error
svm_tree <- svm(Surv(Time, Event)~.-PlantDateYear, data=tree)

# Other caret methods testing
svm_tree <- train(Surv(Time, Event)~.-PlantDateYear, data=tree %>% na.omit(), method="svmPoly")
svm_tree <- train(Surv(Time, Event)~.-PlantDateYear, data=tree %>% na.omit(), method="lssvmRadial")
svm_tree <- train(Surv(Time, Event)~.-PlantDateYear, data=tree %>% na.omit(), method="lssvmPoly")
svm_tree <- train(Surv(Time, Event)~.-PlantDateYear, data=tree %>% na.omit(), method="svmLinear3")
svm_tree <- train(Surv(Time, Event)~.-PlantDateYear, data=tree %>% na.omit(), method="svmBoundrangeString")
```

We can instead use the `survivalsvm` package to predict survival time and receive a concordance index value to compare with other models.

```{r survivalSVM}
set.seed(1)

# Split data
train_rows <- sample(1:nrow(tree), size=nrow(tree)/2)

# Create train and test subsets
train <- tree[train_rows,]
test <- tree[-train_rows,]

# survivalsvm function
svm_tree <- survivalsvm(Surv(Time, Event)~.-PlantDateYear, data=train, type="vanbelle1", diff.meth="makediff1", gamma.mu=1)

# predictions using the resulting object
svm_pred <- predict(svm_tree, newdata=test)

# get c-index value
svm_c <- conindex(svm_pred, test$Time)

svm_c
```


# Single Survival Tree

A survival tree to analyze the relationships between variables in the dataset can be implemented by analyzing splits in the data that yield the largest differences in survival. This can be done by comparing the survival outcomes of different categories in a given variable and determining which traits yield the highest chance of survival.

As with our SVM model, we pass in a survival object into our model for the model to make survival predictions. The results of the predictions made from the tree are shown in 
```{r}
surv_tree <- rpart(Surv(Time, Event) ~.-Census, data = tree, model = TRUE)
rpart.plot(surv_tree, main = "Fig. 5: Single Decision Tree Example")
validate(surv_tree)
```


As shown in Fig. 5, Phenolics and the date that the tree was planted appear to be the variables most linked to tree survival. With use of the "validate" function in the rms package, different control values $k$ were tested and the appropriate $D_{xy}$ values for training and testing were output. The package implements 10-fold cross validation to ensure that the error values accurately represent how well the algorithm might perform on new data. For each attempted $k$, the mean $D_{xy}$ was calculated. One can convert from $D_{xy}$ to the c-index through the relationship
$D_{xy} = 2(C-0.5)$, 
where $C$ is the c-index. 

```{r}
tree_res <- validate(surv_tree)
best_c <- max(tree_res$dxy.val)
# Calculate c-index from testing dxy 
c_ind <- 0.5 + best_c/2
c_ind
1-c_ind
```

As shown above, the c-index is ~0.74, which means that the associated error is 0.26. This result implicates that the model is performing better than random guessing and specifically can predict with 74% accuracy how long it will take for a tree to die given the reported conditions. This method, however, is susceptible to high variance. Because this model is a single decision tree, there is a lot of uncertainty and a few changes in some of the edge observations could change the way that the model is predicting in a significant manner. This tree might be predicting with 74% survival accuracy in this set of observations, but might perform much differently given a slightly different set of observations. 

# Survival Random Forest

A similar model with less variance is the random forest. This model makes use of a collection of "weak-learners" to make predictions on a dataset. These weak-learners are designed to make predictions that are slightly better than random guessing and a majority vote is taken to determine the resulting prediction for a given observation. Because this method uses majority vote, it is subject to less variance than a single tree model. A similar approach can be done for survival analysis on a random forest. Note: 1 observation was omitted, as the reported "event" was unknown. In a similar way as done for the single tree, a survival object is passed into the ranger function and importance is tracked. 
```{r}
rf <- ranger(Surv(Time, Event) ~.-Census, data = tree %>%na.omit(), importance = 'impurity')
rf$prediction.error
rf_c = 1 - rf$prediction.error
rf_c
as.data.frame(rf$variable.importance) %>%
  arrange(-rf$variable.importance)
```
As shown above, the random forest makes survival predictions that yield a c-index of $0.70$. This result implicates that the tree is predicting survival times with $70%$ accuracy (and $30%$ error). This is a worse prediction than the one found above for a single decision tree, however, it is important to reiterate that this model is subject to less bias than the single decision tree and might prove to generalize better to new data. 











